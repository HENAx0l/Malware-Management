from collections import deque
from collections import OrderedDict 

import random
import numpy as np

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.checkpoint import checkpoint
from secml.array import CArray

from attack.LowMemConv_secml import LowMemConvBase
from attack_space_management import Management

def getParams():
    #Format for this is to make it work easily with Optuna in an automated fashion.
    #variable name -> tuple(sampling function, dict(sampling_args) )
    params = {
        'channels'     : ("suggest_int", {'name':'channels', 'low':32, 'high':1024}),
        'log_stride'   : ("suggest_int", {'name':'log2_stride', 'low':2, 'high':9}),
        'window_size'  : ("suggest_int", {'name':'window_size', 'low':32, 'high':512}),
        'embd_size'    : ("suggest_int", {'name':'embd_size', 'low':4, 'high':64}),
    }
    return OrderedDict(sorted(params.items(), key=lambda t: t[0]))

def initModel(**kwargs):
    new_args = {}
    for x in getParams():
        if x in kwargs:
            new_args[x] = kwargs[x]
            
    return MalConv(**new_args)


class MalConv(LowMemConvBase):
    
    def __init__(self, out_size=2, channels=128, window_size=512, stride=512, embd_size=8, log_stride=None,mode='default'):
        self.embedding_size=embd_size
        super(MalConv, self).__init__()
        self.embd = nn.Embedding(257, embd_size, padding_idx=0)
        if not log_stride is None:
            stride = 2**log_stride
        self.mode=mode
        self.conv_1 = nn.Conv1d(embd_size, channels, window_size, stride=stride, bias=True)
        self.conv_2 = nn.Conv1d(embd_size, channels, window_size, stride=stride, bias=True)

        
        self.fc_1 = nn.Linear(channels, channels)
        self.fc_2 = nn.Linear(channels, out_size)
        
    
    def processRange(self, x):
        x = self.embd(x)
        x = torch.transpose(x,-1,-2)
        
        cnn_value = self.conv_1(x)
        gating_weight = torch.sigmoid(self.conv_2(x))
        
        x = cnn_value * gating_weight
        
        return x
    
    def forward(self, x):
        
        inner_array = x.squeeze().cpu().numpy()
        pebyte = (inner_array[inner_array != 666]-1).astype(np.uint8).tobytes()
        if self.mode=='default':
            x=pebyte
        else:
            m=Management(pebyte)
            m.scan()
            x=m.bytearray_management(self.mode)
        x = np.frombuffer(x[:self.max_input_size], dtype=np.uint8).astype(np.int16)+1
        x=np.pad(x, (0, self.max_input_size - len(x)), constant_values=1)
        x=torch.tensor(x).unsqueeze(0)
        post_conv = x = self.seq2fix(x)
        
        penult = x = F.relu(self.fc_1(x))
        x = self.fc_2(x)
        #return x, penult, post_conv
        return F.softmax(x, dim=-1)[0][1].unsqueeze(0).unsqueeze(0)



